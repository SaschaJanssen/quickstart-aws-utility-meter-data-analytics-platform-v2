// Include any postdeployment steps here, such as steps necessary to test that the deployment was successful. If there are no postdeployment steps, leave this file empty.
:xrefstyle: short

== Postdeployment steps

To see the capabilities of the QuickStart, follow the steps in this section to. These are additional information to dive deeper into the QuickStart, and adopt the original code basis.

=== Set up Amazon Managed Grafana dashboards
This solution includes a set of Amazon Managed Grafana dashboards. These dashboards use Amazon Athena to query data stored in a data lake and depend on several of the optional datasets for full functionality.

NOTE: Amazon Managed Grafana supports Security Assertion Markup Language (SAML) and AWS Identity and Access Management (IAM) Identity Center for authentication. If your organization does not have SAML or AWS IAM Identity Center set up, consult your administrator.

==== Create a Grafana workspace
. In the https://console.aws.amazon.com/grafana[Amazon Managed Grafana] console, choose *Create workspace*.

. Enter a *Workspace Name* (for example, `mda_solution`). If desired, enter an optional *Workspace Description*.
. Choose *Next*.
. On the *Configure Settings* page, for *Authentication access*, select *Security Assertion Markup Language (SAML)*.
. For *Permission type*, choose *Service managed*.
. Choose *Next*.
. On the *Service managed permission settings* page, for *IAM permission access settings*, choose *Current account*.
. For *Data sources and notification channels*, select *Amazon Athena*.
. Choose *Next*.
. Choose *Create workspace*.

==== Select SSO users and groups
After creating a Grafana workspace, select the single sign-on (SSO) users and groups you want to give access to it. For more information, refer to https://docs.aws.amazon.com/grafana/latest/userguide/AMG-manage-users-and-groups-AMG.html[Managing user and group access to Amazon Managed Grafana].

To access the workspace, users sign in to the *Grafana workspace URL* found on *Workspaces* page in the https://console.aws.amazon.com/grafana[Amazon Managed Grafana] console.

==== Set up Amazon Athena as a data source in Grafana
Add Amazon Athena as a data source in Amazon Managed Grafana. For more information, refer to https://docs.aws.amazon.com/grafana/latest/userguide/AWS-Athena.html[Amazon Athena] and https://grafana.com/docs/grafana/latest/administration/data-source-management/[Data Source Management].

==== Import dashboards into Amazon Managed Grafana

Upload or paste the contents of dashboard JSON files from the `/scripts/assets/grafana` folder of the https://fwd.aws/z8dwV?[solution's repository] to Amazon Managed Grafana. For more information, refer to https://docs.aws.amazon.com/grafana/latest/userguide/dashboard-export-and-import.html#importing-a-dashboard[Importing a dashboard].

After importing, you may need to verify that the names of dashboard panel data sources match AWS Glue database names. If you receive errors, check panel data sources and variables in dashboard settings. For more information, refer to https://docs.aws.amazon.com/grafana/latest/userguide/add-a-panel-to-a-dashboard.html[Adding or editing a panel] and https://docs.aws.amazon.com/grafana/latest/userguide/dashboard-overview.html[Dashboards].

=== Input adapter
As shown in <<inputadapter>> the solution's input adapter loads meter reads from an external source (such as HES or FTP) and prepares them for processing.

[#inputadapter]
.Input adapter
[link=images/input_adapter.png]
image::../docs/deployment_guide/images/input_adapter.png[Inputadapter]

A step functions workflow orchestrates the generation and download of the meter-reads file as a compressed file from the external database to the inbound bucket. Another process extracts the file from the file storage bucket and stores it in the uncompressed folder.

The inbound bucket holds the compressed and uncompressed files. The solution deletes uncompressed files to save storage and costs.

After the file is extracted, the solution generates an event that invokes a Lambda function for further processing.

The file-range-extractor Lambda function extracts range information (a range is a group of lines which should be processed together) from the uncompressed file based on the file size and number of chunks (configurable). A range is a group of lines that you want to process together. Each range information will be sent to SQS.

Each worker takes a range from the queue and processes the respective meter reads (parse and transform) before sending each element to Amazon Kinesis. This process ensures that the content input file can be processed in parallel. The worker transform the CSV line into JSON, and creates a separate object for each reading type.

The Amazon Kinesis data stream is used to ingest the data into the staging area. The stream scales on-demand.


You can set up connections to other data sources by configuring additional dataflows. A dataflow connects to the external database, loads data from it, and stores data in a purpose-built database that can be accessed from the solution's central Data Catalog.

The solution comes with two sample dataflows: weather and topology. To add a new dataflow, create a data pipeline that loads data from the source, prepares them, and stores results in an appropriate data store. Then add the data store you've configured to the solution's Data Catalog.
=== Dataflows


The architecture shows an example implementation, services can change depending on the requirements.

.custom dataflow
image::../docs/deployment_guide/images/custom_dataflow.png[Architecture,width=80%,height=80%]

=== Data partitioning
The curated data in the *integration stage* S3 bucket is partitioned by *reading type, year, month, day, hour*, as follows:

`s3://IntegrationBucket/reading_type=<reading_type_value>/year=<year>/month=<month>/day=<day>/hour=<hour>/<meter-data-file-in-parquet-format>`

You can find all meter reads for the hour of a day on the lowest level of the partition tree. To optimize query performance, the data is stored in a column-based file format (Parquet).

=== Late-arriving data
The data lake handles late-arriving meter reads, will be detected as soon as the data reaches the *staging stage*. If a late read is detected an event will be send to Amazon EventBridge. The ETL pipeline takes care of moving the late read to the correct partition, and makes sure that data is still stored in an optimized way.

=== Data Formats
*Inbound format*

The input meter-data format is variable and can be adjusted as described in the section <<Customize this QuickStart,'Customize this Quick Start'>>. The sample input data format of the https://github.com/aws-quickstart/quickstart-aws-utility-meter-data-generator[Meter Data Generator] looks like the following:


[cols="1,1,1", options="header"]
.Inbound schema
|===
|Field
|Type
|Format
|Description

|time|timestamp|yyyy-MM-dd HH:mm:ss.SSSSSSS|The timestamp the read reaches the source system
|reading_time|timestamp|yyyy-MM-dd HH:mm:ss.SSSSSSS|The timestamp of the actual read
|device_id|string|7a044be7-2f1e-3bf1-aa86-b8b1b9064f19|uuid
|measure_name|string|
|load|double|0.000|Load, unit: A
|crrnt|double|0.000|Current, unit: A
|pf|double|0.000|Power Factor, between 0 and 1
|kva|double|0.000|Volt Ampere, unit: VA
|kw|double|0.000|Kilowatt, unit: kW
|vltg|double|0.000|Voltage, unit V
|===

*Integrated format*

Data is stored in the following format in the *integration stage*.

[cols="1,1,1", options="header"]
.Integration schema
|===
|Field
|Type
|Format

|meter_id           |String     |
|reading_value      |Double     |0.000
|reading_date_time  |Timestamp  |yyyy-MM-dd HH:mm:ss.SSS
|reading_type       |String     |load, crrnt, pf, kva, kw, vltg
|unit               |String     |
|obis_code          |String     |
|phase              |String     |
|reading_source     |String     |
|reading_type       |String (Partitioned)   |
|year               |String (Partitioned)   |
|month              |String (Partitioned)   |
|day                |String (Partitioned)   |
|hour               |String (Partitioned)   |
|===
